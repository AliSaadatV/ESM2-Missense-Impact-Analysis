{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a18a68d7",
   "metadata": {},
   "source": [
    "The goal of this is to apply the fine-tuned model on all human proteins w/o annotations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f08e92",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150016cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dependencies\n",
    "import os.path\n",
    "#os.chdir(\"set working path here\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "import transformers, datasets\n",
    "from transformers.modeling_outputs import TokenClassifierOutput\n",
    "from transformers.models.t5.modeling_t5 import T5Config, T5PreTrainedModel, T5Stack\n",
    "from transformers.utils.model_parallel_utils import assert_device_map, get_device_map\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "from transformers.models.esm.modeling_esm import EsmPreTrainedModel, EsmModel\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TrainingArguments, Trainer, set_seed\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "# for custom DataCollator\n",
    "from transformers.data.data_collator import DataCollatorMixin\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
    "from transformers.utils import PaddingStrategy\n",
    "\n",
    "import peft\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, inject_adapter_in_model, LoraConfig\n",
    "\n",
    "from evaluate import load\n",
    "from datasets import Dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14d7d448-d4a0-47fb-b57c-b3fea09d33af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version:  2.5.1+cu124\n",
      "Cuda version:  12.4\n",
      "Numpy version:  2.0.2\n",
      "Pandas version:  2.2.3\n",
      "Transformers version:  4.51.3\n",
      "Datasets version:  3.1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Torch version: \",torch.__version__)\n",
    "print(\"Cuda version: \",torch.version.cuda)\n",
    "print(\"Numpy version: \",np.__version__)\n",
    "print(\"Pandas version: \",pd.__version__)\n",
    "print(\"Transformers version: \",transformers.__version__)\n",
    "print(\"Datasets version: \",datasets.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4cabd14-3fc5-4cf0-8be5-bd0fbdaaa043",
   "metadata": {},
   "outputs": [],
   "source": [
    "ESMs = [\"facebook/esm2_t6_8M_UR50D\",\n",
    "         \"facebook/esm2_t12_35M_UR50D\",\n",
    "         \"facebook/esm2_t30_150M_UR50D\",\n",
    "         \"facebook/esm2_t33_650M_UR50D\",\n",
    "         \"facebook/esm2_t36_3B_UR50D\"]\n",
    "\n",
    "ProtT5 = [\"Rostlab/prot_t5_xl_uniref50\"]\n",
    "\n",
    "selected_checkpoint = \"facebook/esm2_t36_3B_UR50D\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cb4846",
   "metadata": {},
   "source": [
    "# model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b729ad9-dc0d-4af8-8c0e-05d153e7e7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EsmForTokenClassificationCustom(EsmPreTrainedModel):\n",
    "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.esm = EsmModel(config, add_pooling_layer=False)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, TokenClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.esm(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        # changed to ignore special tokens at the seq start and end \n",
    "        # as well as invalid positions (labels -100)\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "\n",
    "            active_loss = attention_mask.view(-1) == 1\n",
    "            active_logits = logits.view(-1, self.num_labels)\n",
    "\n",
    "            active_labels = torch.where(\n",
    "              active_loss, labels.view(-1), torch.tensor(-100).type_as(labels)\n",
    "            )\n",
    "\n",
    "            valid_logits=active_logits[active_labels!=-100]\n",
    "            valid_labels=active_labels[active_labels!=-100]\n",
    "            \n",
    "            valid_labels=valid_labels.type(torch.LongTensor).to('cuda:0')\n",
    "            \n",
    "            loss = loss_fct(valid_logits, valid_labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return TokenClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "    \n",
    "# based on transformers DataCollatorForTokenClassification\n",
    "@dataclass\n",
    "class DataCollatorForTokenClassificationESM(DataCollatorMixin):\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received, as well as the labels.\n",
    "    Args:\n",
    "        tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):\n",
    "            The tokenizer used for encoding the data.\n",
    "        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            - `True` or `'longest'` (default): Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence is provided).\n",
    "            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
    "              acceptable input length for the model if that argument is not provided.\n",
    "            - `False` or `'do_not_pad'`: No padding (i.e., can output a batch with sequences of different lengths).\n",
    "        max_length (`int`, *optional*):\n",
    "            Maximum length of the returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (`int`, *optional*):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "        label_pad_token_id (`int`, *optional*, defaults to -100):\n",
    "            The id to use when padding the labels (-100 will be automatically ignore by PyTorch loss functions).\n",
    "        return_tensors (`str`):\n",
    "            The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    label_pad_token_id: int = -100\n",
    "    return_tensors: str = \"pt\"\n",
    "\n",
    "    def torch_call(self, features):\n",
    "        import torch\n",
    "\n",
    "        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "        labels = [feature[label_name] for feature in features] if label_name in features[0].keys() else None\n",
    "\n",
    "        no_labels_features = [{k: v for k, v in feature.items() if k != label_name} for feature in features]\n",
    "\n",
    "        batch = self.tokenizer.pad(\n",
    "            no_labels_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        if labels is None:\n",
    "            return batch\n",
    "\n",
    "        sequence_length = batch[\"input_ids\"].shape[1]\n",
    "        padding_side = self.tokenizer.padding_side\n",
    "\n",
    "        def to_list(tensor_or_iterable):\n",
    "            if isinstance(tensor_or_iterable, torch.Tensor):\n",
    "                return tensor_or_iterable.tolist()\n",
    "            return list(tensor_or_iterable)\n",
    "\n",
    "        if padding_side == \"right\":\n",
    "            batch[label_name] = [\n",
    "                # to_list(label) + [self.label_pad_token_id] * (sequence_length - len(label)) for label in labels\n",
    "                # changed to pad the special tokens at the beginning and end of the sequence\n",
    "                [self.label_pad_token_id] + to_list(label) + [self.label_pad_token_id] * (sequence_length - len(label)-1) for label in labels\n",
    "            ]\n",
    "        else:\n",
    "            batch[label_name] = [\n",
    "                [self.label_pad_token_id] * (sequence_length - len(label)) + to_list(label) for label in labels\n",
    "            ]\n",
    "\n",
    "        batch[label_name] = torch.tensor(batch[label_name], dtype=torch.float)\n",
    "        return batch\n",
    "\n",
    "def _torch_collate_batch(examples, tokenizer, pad_to_multiple_of: Optional[int] = None):\n",
    "    \"\"\"Collate `examples` into a batch, using the information in `tokenizer` for padding if necessary.\"\"\"\n",
    "    import torch\n",
    "\n",
    "    # Tensorize if necessary.\n",
    "    if isinstance(examples[0], (list, tuple, np.ndarray)):\n",
    "        examples = [torch.tensor(e, dtype=torch.long) for e in examples]\n",
    "\n",
    "    length_of_first = examples[0].size(0)\n",
    "\n",
    "    # Check if padding is necessary.\n",
    "\n",
    "    are_tensors_same_length = all(x.size(0) == length_of_first for x in examples)\n",
    "    if are_tensors_same_length and (pad_to_multiple_of is None or length_of_first % pad_to_multiple_of == 0):\n",
    "        return torch.stack(examples, dim=0)\n",
    "\n",
    "    # If yes, check if we have a `pad_token`.\n",
    "    if tokenizer._pad_token is None:\n",
    "        raise ValueError(\n",
    "            \"You are attempting to pad samples but the tokenizer you are using\"\n",
    "            f\" ({tokenizer.__class__.__name__}) does not have a pad token.\"\n",
    "        )\n",
    "\n",
    "    # Creating the full tensor and filling it with our data.\n",
    "    max_length = max(x.size(0) for x in examples)\n",
    "    if pad_to_multiple_of is not None and (max_length % pad_to_multiple_of != 0):\n",
    "        max_length = ((max_length // pad_to_multiple_of) + 1) * pad_to_multiple_of\n",
    "    result = examples[0].new_full([len(examples), max_length], tokenizer.pad_token_id)\n",
    "    for i, example in enumerate(examples):\n",
    "        if tokenizer.padding_side == \"right\":\n",
    "            result[i, : example.shape[0]] = example\n",
    "        else:\n",
    "            result[i, -example.shape[0] :] = example\n",
    "    return result\n",
    "\n",
    "def tolist(x):\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    elif hasattr(x, \"numpy\"):  # Checks for TF tensors without needing the import\n",
    "        x = x.numpy()\n",
    "    return x.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8da5dd0",
   "metadata": {},
   "source": [
    "# functions to load the fine-tuned weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e9f2450-8d5c-4874-bd4a-32739758adf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load ESM2 models\n",
    "def load_esm_model_classification(checkpoint, num_labels, half_precision, full=False, deepspeed=False):\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    \n",
    "    if half_precision and deepspeed:\n",
    "        model = EsmForTokenClassificationCustom.from_pretrained(checkpoint, num_labels = num_labels, torch_dtype = torch.float16)\n",
    "    else:\n",
    "        model = EsmForTokenClassificationCustom.from_pretrained(checkpoint, num_labels = num_labels)\n",
    "        \n",
    "    if full == True:\n",
    "        return model, tokenizer \n",
    "        \n",
    "    peft_config = LoraConfig(\n",
    "        r=4, lora_alpha=1, bias=\"all\", target_modules=[\"query\",\"key\",\"value\",\"dense\"]\n",
    "    )\n",
    "    \n",
    "    model = inject_adapter_in_model(peft_config, model)\n",
    "    \n",
    "    # Unfreeze the prediction head\n",
    "    for (param_name, param) in model.classifier.named_parameters():\n",
    "                param.requires_grad = True  \n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5fbdca2-6601-4c29-a181-63186ad2858d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(checkpoint, filepath, num_labels=2, half_precision = True, full = False, deepspeed=False):\n",
    "# Creates a new PT5 model and loads the finetuned weights from a file\n",
    "#example: tokenizer, model_reload = load_model(checkpoint, f\"./fine_tuned_models/{checkpoint}/{all_features_re[0]}.pth\", num_labels=2)\n",
    "    \n",
    "    # load model\n",
    "    if \"esm\" in checkpoint:\n",
    "        model, tokenizer = load_esm_model_classification(checkpoint, num_labels, half_precision, full, deepspeed)\n",
    "    else:\n",
    "        model, tokenizer = load_T5_model_classification(checkpoint, num_labels, half_precision, full, deepspeed)\n",
    "    \n",
    "    # Load the non-frozen parameters from the saved file\n",
    "    non_frozen_params = torch.load(filepath)\n",
    "\n",
    "    # Assign the non-frozen parameters to the corresponding parameters of the model\n",
    "    for param_name, param in model.named_parameters():\n",
    "        if param_name in non_frozen_params:\n",
    "            param.data = non_frozen_params[param_name].data\n",
    "\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84c9bdbe-af23-499e-95ed-a4e33df1665a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(        Entry  Length                                           Sequence  \\\n",
       " 0  A0A087X1C5     515  MGLEALVPLAMIVAIFLLLVDLMHRHQRWAARYPPGPLPLPGLGNL...   \n",
       " 1  A0A0B4J2F0      54  MFRRLTFAQLLFATVLGIAGGVYIFQPVFEQYAKDQKELKEKMQLV...   \n",
       " 2  A0A0B4J2F2     783  MVIMSEFSADPAGQGQGQQKPLRVGFYDIERTLGKGNFAVVKLARH...   \n",
       " 3  A0A0C5B5G6      16                                   MRWQEMGYIFYPRKLR   \n",
       " 4  A0A0K2S4Q6     201  MTQRAGAAMLPSALLLLCVPGCLTVSGPSTVMGAVGESLSVQCRYE...   \n",
       " \n",
       "   Gene Names (primary)                                   Protein families  \\\n",
       " 0               CYP2D7                             Cytochrome P450 family   \n",
       " 1              PIGBOS1                                                NaN   \n",
       " 2                SIK1B  Protein kinase superfamily, CAMK Ser/Thr prote...   \n",
       " 3              MT-RNR1                                                NaN   \n",
       " 4               CD300H                                       CD300 family   \n",
       " \n",
       "                                          Active site  \\\n",
       " 0                                                NaN   \n",
       " 1                                                NaN   \n",
       " 2  ACT_SITE 149; /note=\"Proton acceptor\"; /eviden...   \n",
       " 3                                                NaN   \n",
       " 4                                                NaN   \n",
       " \n",
       "                                         Binding site DNA binding Site  \\\n",
       " 0  BINDING 461; /ligand=\"heme\"; /ligand_id=\"ChEBI...         NaN  NaN   \n",
       " 1                                                NaN         NaN  NaN   \n",
       " 2  BINDING 33..41; /ligand=\"ATP\"; /ligand_id=\"ChE...         NaN  NaN   \n",
       " 3                                                NaN         NaN  NaN   \n",
       " 4                                                NaN         NaN  NaN   \n",
       " \n",
       "   Intramembrane  ... Beta strand Helix Turn Coiled coil  \\\n",
       " 0           NaN  ...         NaN   NaN  NaN         NaN   \n",
       " 1           NaN  ...         NaN   NaN  NaN         NaN   \n",
       " 2           NaN  ...         NaN   NaN  NaN         NaN   \n",
       " 3           NaN  ...         NaN   NaN  NaN         NaN   \n",
       " 4           NaN  ...         NaN   NaN  NaN         NaN   \n",
       " \n",
       "                                   Compositional bias  \\\n",
       " 0                                                NaN   \n",
       " 1                                                NaN   \n",
       " 2  COMPBIAS 457..477; /note=\"Polar residues\"; /ev...   \n",
       " 3                                                NaN   \n",
       " 4                                                NaN   \n",
       " \n",
       "                                          Domain [FT] Motif  \\\n",
       " 0                                                NaN   NaN   \n",
       " 1                                                NaN   NaN   \n",
       " 2  DOMAIN 27..278; /note=\"Protein kinase\"; /evide...   NaN   \n",
       " 3                                                NaN   NaN   \n",
       " 4  DOMAIN 25..123; /note=\"Ig-like V-type\"; /evide...   NaN   \n",
       " \n",
       "                                               Region Repeat Zinc finger  \n",
       " 0                                                NaN    NaN         NaN  \n",
       " 1  REGION 30..36; /note=\"Required for interaction...    NaN         NaN  \n",
       " 2  REGION 353..377; /note=\"Disordered\"; /evidence...    NaN         NaN  \n",
       " 3                                                NaN    NaN         NaN  \n",
       " 4                                                NaN    NaN         NaN  \n",
       " \n",
       " [5 rows x 32 columns],\n",
       " 20434)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/uniprot_all_human_proteins.txt.gz\", sep='\\t')\n",
    "df.head(), len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829349df",
   "metadata": {},
   "source": [
    "# functions to convert the uniprot text into labels (for annotated proteins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59298d58-14c4-4b7a-ae02-fa4972cc74df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_labels_region(sequence, feature, feature_re):\n",
    "    # Start with all 0s\n",
    "    labels = np.zeros(len(sequence), dtype=np.int64)\n",
    "    region_re = f\"{feature_re}\\s(\\d+)\\.\\.(\\d+)\\;\"\n",
    "    residue_re = f'{feature_re}\\s(\\d+);'\n",
    "\n",
    "    found_region = re.findall(region_re, feature)\n",
    "\n",
    "    for start, end in found_region:\n",
    "        start = int(start) - 1\n",
    "        end = int(end)\n",
    "        assert end <= len(sequence)\n",
    "        labels[start: end] = 1\n",
    "\n",
    "    found_residue = re.findall(residue_re, feature)\n",
    "    for pos in found_residue:\n",
    "        pos = int(pos) -1\n",
    "        assert pos <= len(sequence)\n",
    "        labels[pos] = 1\n",
    "\n",
    "    return ''.join(map(str, labels))\n",
    "\n",
    "\n",
    "def build_labels_bonds(sequence, feature, feature_re):\n",
    "    # Start with all 0s\n",
    "    labels = np.zeros(len(sequence), dtype=np.int64)\n",
    "\n",
    "    region_re = f\"{feature_re}\\s(\\d+)\\.\\.(\\d+)\\;\"\n",
    "\n",
    "    if isinstance(feature, float): # Indicates missing (NaN)\n",
    "        found_feature = []\n",
    "    else:\n",
    "        found_feature = re.findall(region_re, feature)\n",
    "    for start, end in found_feature:\n",
    "        start = int(start) - 1\n",
    "        end = int(end) -1\n",
    "        assert end <= len(sequence)\n",
    "        labels[start] = 1\n",
    "        labels[end] = 1 \n",
    "    return ''.join(map(str, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a927e59-7a75-4bcc-917f-f9e7edf1f67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_label(sequence, feature, feature_re, colname):\n",
    "  if colname == 'Disulfide bond':\n",
    "    return build_labels_bonds(sequence, feature, feature_re)\n",
    "  else:\n",
    "    return build_labels_region(sequence, feature, feature_re)\n",
    "\n",
    "\n",
    "# Function to apply to each row\n",
    "def process_row(row, colname, feature_re):\n",
    "    if pd.isna(row[colname]):\n",
    "        return pd.Series(['pred', np.nan])\n",
    "    else:\n",
    "        return pd.Series(['actual', generate_label(row['Sequence'], row[colname], feature_re, colname)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9980f3a5-ed3f-4d97-8c76-b3195456a8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = ['Active site', 'Binding site', 'DNA binding', \n",
    "                'Topological domain', 'Transmembrane',\n",
    "                'Disulfide bond', 'Modified residue', 'Propeptide', 'Signal peptide', 'Transit peptide',\n",
    "                'Beta strand', 'Helix', 'Turn',\n",
    "                'Coiled coil', 'Compositional bias', 'Domain [FT]', 'Motif', 'Region', 'Repeat', 'Zinc finger']\n",
    "\n",
    "all_features_re = ['ACT_SITE', 'BINDING', 'DNA_BIND', \n",
    "                   'TOPO_DOM', 'TRANSMEM',\n",
    "                   'DISULFID', 'MOD_RES',  'PROPEP', 'SIGNAL', 'TRANSIT',\n",
    "                   'STRAND', 'HELIX', 'TURN',\n",
    "                   'COILED', 'COMPBIAS', 'DOMAIN', 'MOTIF', 'REGION', 'REPEAT', 'ZN_FING']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b533291d-eea8-4275-9330-afd85dd5605b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual\n",
    "for i in range(len(all_features)):\n",
    "  colname = all_features[i]\n",
    "  feature_re = all_features_re[i]\n",
    "\n",
    "  # Apply the function and assign new columns\n",
    "  df[[f'actual_or_pred_{feature_re}', f'annotation_actual_{feature_re}']] = df.apply(lambda row: process_row(row, colname, feature_re), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06d80ab2-5d91-40b1-8886-49285c4fdb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split long sequences for ESM2 inference (max size = 1022 AA)\n",
    "def split_text(row, n=1022):\n",
    "    text = row['Sequence']\n",
    "    if len(text) <= n:\n",
    "        # No split needed, return the row with an added 'split_indicator' column\n",
    "        return pd.DataFrame({**row, 'split_indicator': [0]}).iloc[[0]]\n",
    "    else:\n",
    "        # Split needed, generate parts and a corresponding DataFrame\n",
    "        splits = [text[i:i+n] for i in range(0, len(text), n)]\n",
    "        part_data = {col: [row[col]] * len(splits) if col != 'Sequence' else splits for col in df.columns}\n",
    "        part_data['split_indicator'] = range(1, len(splits) + 1)  # numbering the splits\n",
    "        return pd.DataFrame(part_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4a6ff2",
   "metadata": {},
   "source": [
    "# run inference for all features and store them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb627ef-7e3f-45f8-88d0-647dff6de5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e960c7205a452086fb8aae5944e40c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForTokenClassificationCustom were not initialized from the model checkpoint at facebook/esm2_t36_3B_UR50D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/867706/ipykernel_2776704/3814221467.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  non_frozen_params = torch.load(filepath)\n",
      "Predicting:   0%|                                                                                                                                                                        | 0/20813 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Predicting: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20813/20813 [52:39<00:00,  6.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "660cb44f3d1f403c8c52d2298a81161e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForTokenClassificationCustom were not initialized from the model checkpoint at facebook/esm2_t36_3B_UR50D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/867706/ipykernel_2776704/3814221467.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  non_frozen_params = torch.load(filepath)\n",
      "Predicting:   0%|                                                                                                                                                                        | 0/17964 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Predicting: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17964/17964 [44:00<00:00,  6.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df04c9d07de24ff6a4622cd05aea1a84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForTokenClassificationCustom were not initialized from the model checkpoint at facebook/esm2_t36_3B_UR50D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/867706/ipykernel_2776704/3814221467.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  non_frozen_params = torch.load(filepath)\n",
      "Predicting:   0%|                                                                                                                                                                        | 0/22810 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Predicting:  32%|██████████████████████████████████████████████████▏                                                                                                          | 7290/22810 [19:14<28:59,  8.92it/s]"
     ]
    }
   ],
   "source": [
    "def single_inference(model, tokenizer, aa_seq, checkpoint):\n",
    "    # Preprocess the input sequence\n",
    "    aa_seq = aa_seq.replace(\"O\", \"X\").replace(\"B\", \"X\").replace(\"U\", \"X\").replace(\"Z\", \"X\")\n",
    "    \n",
    "    if \"prot_t5\" in checkpoint:\n",
    "        aa_seq = \" \".join(aa_seq)\n",
    "\n",
    "    # Set the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Tokenize input\n",
    "    encoded = tokenizer(\n",
    "        aa_seq,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"longest\",\n",
    "        truncation=True,\n",
    "        is_split_into_words=False,\n",
    "    )\n",
    "\n",
    "    # Move inputs to device\n",
    "    input_ids = encoded[\"input_ids\"].to(device)\n",
    "    attention_mask = encoded[\"attention_mask\"].to(device)\n",
    "\n",
    "    # Forward pass\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits  # shape: (1, seq_len, num_classes)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "    return probs[:,:,1].squeeze().tolist()  #probability of being 1\n",
    "\n",
    "\n",
    "\n",
    "# pred\n",
    "for i in range(len(all_features)):\n",
    "\n",
    "    print(f'i = {i}')\n",
    "    colname = all_features[i]\n",
    "    feature_re = all_features_re[i]\n",
    "\n",
    "    # load model\n",
    "    finetuned_params_path = f'../res/models/ft_{feature_re}_{selected_checkpoint.split(\"/\")[1]}.pth'\n",
    "    tokenizer, model = load_model(selected_checkpoint, finetuned_params_path)\n",
    "    \n",
    "    df_pred = df[df[colname].isna()].copy()\n",
    "    split_df = df_pred.apply(split_text, axis=1)\n",
    "    df_pred = pd.concat([item for item in split_df]).reset_index(drop=True)\n",
    "\n",
    "    temp_results_list = []\n",
    "    for aa_seq in tqdm(df_pred[\"Sequence\"].values, desc=\"Predicting\"):\n",
    "        probs = single_inference(model, tokenizer, aa_seq, selected_checkpoint)  # list of probabilities\n",
    "        pred_str = ''.join(['1' if p > 0.5 else '0' for p in probs])\n",
    "        temp_results_list.append(pred_str)\n",
    "\n",
    "    df_pred[f'annotation_pred_{feature_re}'] = temp_results_list\n",
    "    df_pred = df_pred[['Entry', f'annotation_pred_{feature_re}']]\n",
    "    df_pred = df_pred.groupby('Entry')[f'annotation_pred_{feature_re}'].agg(''.join).reset_index()\n",
    "    df = pd.merge(df, df_pred, on='Entry', how='left')\n",
    "\n",
    "    df[['Entry', f'actual_or_pred_{feature_re}', f'annotation_actual_{feature_re}', f'annotation_pred_{feature_re}']].to_csv(\n",
    "        f'../res/proteome_inference/uniprot_all_human_proteins_annotated_{feature_re}.txt.gz',\n",
    "        sep='\\t', index=False\n",
    "    )\n",
    "\n",
    "\n",
    "df.to_csv('../res/proteome_inference/uniprot_all_human_proteins_annotated.txt.gz', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b4a5b3-53f5-4464-b104-e2bfe0290b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' batch inference\n",
    "\n",
    "def batch_inference(model, tokenizer, sequences, checkpoint, batch_size=16):\n",
    "    # Preprocess sequences\n",
    "    sequences = [\n",
    "        seq.replace(\"O\", \"X\").replace(\"B\", \"X\").replace(\"U\", \"X\").replace(\"Z\", \"X\")\n",
    "        for seq in sequences\n",
    "    ]\n",
    "    \n",
    "    if \"prot_t5\" in checkpoint:\n",
    "        sequences = [\" \".join(seq) for seq in sequences]\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_probs = []\n",
    "\n",
    "    for i in tqdm(range(0, len(sequences), batch_size), desc=\"Batch inference\"):\n",
    "        batch_seqs = sequences[i:i+batch_size]\n",
    "\n",
    "        # Tokenize\n",
    "        encoded = tokenizer(\n",
    "            batch_seqs,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            is_split_into_words=False,\n",
    "        )\n",
    "\n",
    "        input_ids = encoded[\"input_ids\"].to(device)\n",
    "        attention_mask = encoded[\"attention_mask\"].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "            probs = torch.softmax(logits, dim=-1) # probs.shape: (batch_size, seq_len, num_classes)\n",
    "            all_probs.extend(probs[:, :, 1].cpu().numpy())  # class 1 probability\n",
    "\n",
    "    return all_probs  # list of numpy arrays, one per sequence\n",
    "\n",
    "\n",
    "for i in range(len(all_features)):\n",
    "    print(f'i = {i}')\n",
    "    colname = all_features[i]\n",
    "    feature_re = all_features_re[i]\n",
    "\n",
    "    # load model\n",
    "    finetuned_params_path = f'../res/models/ft_{feature_re}_{selected_checkpoint.split(\"/\")[1]}.pth'\n",
    "    tokenizer, model = load_model(selected_checkpoint, finetuned_params_path)\n",
    "\n",
    "    df_pred = df[df[colname].isna()].copy()\n",
    "    split_df = df_pred.apply(split_text, axis=1)\n",
    "    df_pred = pd.concat([item for item in split_df]).reset_index(drop=True)\n",
    "\n",
    "    seqs = list(df_pred[\"Sequence\"].values)\n",
    "    probs_list = batch_inference(model, tokenizer, seqs, selected_checkpoint, batch_size=16)\n",
    "\n",
    "    temp_results_list = []\n",
    "    for probs in tqdm(probs_list, desc=\"Postprocessing\"):\n",
    "        pred_str = ''.join(['1' if p > 0.5 else '0' for p in probs])\n",
    "        temp_results_list.append(pred_str)\n",
    "\n",
    "    df_pred[f'annotation_pred_{feature_re}'] = temp_results_list\n",
    "    df_pred = df_pred[['Entry', f'annotation_pred_{feature_re}']]\n",
    "    df_pred = df_pred.groupby('Entry')[f'annotation_pred_{feature_re}'].agg(''.join).reset_index()\n",
    "    df = pd.merge(df, df_pred, on='Entry', how='left')\n",
    "\n",
    "    df[['Entry', f'actual_or_pred_{feature_re}', f'annotation_actual_{feature_re}', f'annotation_pred_{feature_re}']].to_csv(\n",
    "        f'../res/proteome_inference/uniprot_all_human_proteins_annotated_{feature_re}.txt.gz',\n",
    "        sep='\\t', index=False\n",
    "    )\n",
    "\n",
    "df.to_csv('../res/proteome_inference/uniprot_all_human_proteins_annotated.txt.gz', sep='\\t', index=False)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
