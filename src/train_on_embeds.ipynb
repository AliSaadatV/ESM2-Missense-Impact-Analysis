{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e34d3510",
   "metadata": {},
   "source": [
    "This notebook was adapted from [https://github.com/RSchmirler/data-repo_plm-finetune-eval/tree/main](https://github.com/RSchmirler/data-repo_plm-finetune-eval/tree/main).\n",
    "\n",
    "The goal is to train a residue classifier using embeddings extracted from pretrained protein language models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e1fcb8",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e12c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, load_from_disk\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import random\n",
    "from sklearn.metrics import (\n",
    "    f1_score, precision_score, recall_score, accuracy_score,\n",
    "    matthews_corrcoef, roc_auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3ce333",
   "metadata": {},
   "source": [
    "Available checkpoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01255240-16b2-4b48-9dde-3839abbce1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ESMs = [ \"esm2_t6_8M_UR50D\" ,\n",
    "         \"esm2_t12_35M_UR50D\" ,\n",
    "         \"esm2_t30_150M_UR50D\" ,\n",
    "         \"esm2_t33_650M_UR50D\",\n",
    "         \"esm2_t36_3B_UR50D\"]\n",
    "\n",
    "ProtT5 = [\"prot_t5_xl_uniref50\"] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7c3ca4",
   "metadata": {},
   "source": [
    "20 features from uniprot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9801cf13-9199-4e69-83b1-1c62c817baf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = ['Active site', 'Binding site', 'DNA binding', \n",
    "                'Topological domain', 'Transmembrane',\n",
    "                'Disulfide bond', 'Modified residue', 'Propeptide', 'Signal peptide', 'Transit peptide',\n",
    "                'Beta strand', 'Helix', 'Turn',\n",
    "                'Coiled coil', 'Compositional bias', 'Domain [FT]', 'Motif', 'Region', 'Repeat', 'Zinc finger']\n",
    "\n",
    "all_features_re = ['ACT_SITE', 'BINDING', 'DNA_BIND', \n",
    "                   'TOPO_DOM', 'TRANSMEM',\n",
    "                   'DISULFID', 'MOD_RES',  'PROPEP', 'SIGNAL', 'TRANSIT',\n",
    "                   'STRAND', 'HELIX', 'TURN',\n",
    "                   'COILED', 'COMPBIAS', 'DOMAIN', 'MOTIF', 'REGION', 'REPEAT', 'ZN_FING']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db21928",
   "metadata": {},
   "source": [
    "Function to extract labels from uniprot text descriptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92943aec-ec28-4cf9-b54a-92ad8e189cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_labels_region(sequence, feature, feature_re):\n",
    "    # Start with all 0s\n",
    "    labels = np.zeros(len(sequence), dtype=np.int64)\n",
    "    region_re = f\"{feature_re}\\s(\\d+)\\.\\.(\\d+)\\;\"\n",
    "    residue_re = f'{feature_re}\\s(\\d+);'\n",
    "\n",
    "    found_region = re.findall(region_re, feature)\n",
    "\n",
    "    for start, end in found_region:\n",
    "        start = int(start) - 1\n",
    "        end = int(end)\n",
    "        assert end <= len(sequence)\n",
    "        labels[start: end] = 1\n",
    "\n",
    "    found_residue = re.findall(residue_re, feature)\n",
    "    for pos in found_residue:\n",
    "        pos = int(pos) -1\n",
    "        assert pos <= len(sequence)\n",
    "        labels[pos] = 1\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def build_labels_bonds(sequence, feature, feature_re):\n",
    "    # Start with all 0s\n",
    "    labels = np.zeros(len(sequence), dtype=np.int64)\n",
    "\n",
    "    region_re = f\"{feature_re}\\s(\\d+)\\.\\.(\\d+)\\;\"\n",
    "\n",
    "    if isinstance(feature, float): # Indicates missing (NaN)\n",
    "        found_feature = []\n",
    "    else:\n",
    "        found_feature = re.findall(region_re, feature)\n",
    "    for start, end in found_feature:\n",
    "        start = int(start) - 1\n",
    "        end = int(end) -1\n",
    "        assert end <= len(sequence)\n",
    "        labels[start] = 1\n",
    "        labels[end] = 1 \n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548f0a8a",
   "metadata": {},
   "source": [
    "# Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e7f860e-bb8b-4432-81d9-4673021862e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "# Implementation of the linear layer\n",
    "class EmbPredictor(nn.Module):      \n",
    "    def __init__(self, input_dim, dense, dropout):\n",
    "        super().__init__()\n",
    "        self.normalizer = nn.BatchNorm1d(input_dim)\n",
    "        self.fc1 = nn.Linear(input_dim, dense)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.output = nn.Linear(dense, 2)  # 2 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.normalizer(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.output(x)  # logits\n",
    "\n",
    "\n",
    "# function to train the model\n",
    "def train_predictor(train_embeds, train_labels,\n",
    "                    val_embeds, val_labels,\n",
    "                    test_embeds, test_labels,\n",
    "                    epochs=10, lr=3e-4, epsilon=1e-7,\n",
    "                    batch=64, dropout=0.2, dense=32, seed=99,\n",
    "                    save_path=\"best_model.pt\", metric_path=\"test_metrics.tsv\"):\n",
    "\n",
    "    set_seed(seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Convert numpy arrays to tensors\n",
    "    X_train = torch.tensor(train_embeds, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(train_labels, dtype=torch.long).squeeze().to(device)\n",
    "\n",
    "    X_val = torch.tensor(val_embeds, dtype=torch.float32).to(device)\n",
    "    y_val = torch.tensor(val_labels, dtype=torch.long).squeeze().to(device)\n",
    "\n",
    "    X_test = torch.tensor(test_embeds, dtype=torch.float32).to(device)\n",
    "    y_test = torch.tensor(test_labels, dtype=torch.long).squeeze().to(device)\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch, shuffle=True)\n",
    "\n",
    "    model = EmbPredictor(X_train.shape[1], dense, dropout).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, eps=epsilon)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_logits = model(X_val)\n",
    "            val_loss = criterion(val_logits, y_val).item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Validation loss: {val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "    # Save best model\n",
    "    if best_model_state is not None:\n",
    "        torch.save(best_model_state, save_path)\n",
    "        print(f\"Best model saved to {save_path} with val loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    # Load best model and evaluate on test\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_logits = model(X_test)\n",
    "        test_probs = torch.softmax(test_logits, dim=1)\n",
    "        test_preds = test_probs.argmax(dim=1).cpu().numpy()\n",
    "        y_test_np = y_test.cpu().numpy()\n",
    "        test_probs_np = test_probs[:, 1].cpu().numpy()  # positive class probs\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics = {\n",
    "        \"f1\": f1_score(y_test_np, test_preds, average='macro'),\n",
    "        \"precision\": precision_score(y_test_np, test_preds, average='macro'),\n",
    "        \"recall\": recall_score(y_test_np, test_preds, average='macro'),\n",
    "        \"mcc\": matthews_corrcoef(y_test_np, test_preds),\n",
    "        \"auroc\": roc_auc_score(y_test_np, test_probs_np, average='macro'),\n",
    "        \"accuracy\": accuracy_score(y_test_np, test_preds)\n",
    "    }\n",
    "\n",
    "    # Save to TSV\n",
    "    df_metrics = pd.DataFrame([metrics])\n",
    "    df_metrics.to_csv(metric_path, sep=\"\\t\", index=False)\n",
    "    print(f\"Test metrics saved to {metric_path}\")\n",
    "\n",
    "    return model, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55904524-8d6c-4377-9629-8e06e7aa6d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all uniprot protein\n",
    "uniprot_df = pd.read_csv(\"../data/uniprot_all_human_proteins.txt.gz\", sep='\\t')\n",
    "long_proteins = uniprot_df[uniprot_df['Length']>1000]['Entry'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2f80b4",
   "metadata": {},
   "source": [
    "# Train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27517a6-5024-4b44-a805-ff959b118486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prot_t5_xl_uniref50\n",
      "Active site\n",
      "Using device: cuda\n",
      "Epoch 1/10 - Validation loss: 0.0018\n",
      "Epoch 2/10 - Validation loss: 0.0019\n",
      "Epoch 3/10 - Validation loss: 0.0020\n",
      "Epoch 4/10 - Validation loss: 0.0021\n",
      "Epoch 5/10 - Validation loss: 0.0029\n",
      "Epoch 6/10 - Validation loss: 0.0025\n",
      "Epoch 7/10 - Validation loss: 0.0024\n",
      "Epoch 8/10 - Validation loss: 0.0028\n",
      "Epoch 9/10 - Validation loss: 0.0026\n",
      "Epoch 10/10 - Validation loss: 0.0041\n",
      "Best model saved to ../res/models/lp_ACT_SITE_prot_t5_xl_uniref50.pt with val loss: 0.0018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/423788/ipykernel_942390/1553744952.py:80: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(save_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics saved to ../res/metrics/lp_ACT_SITE_prot_t5_xl_uniref50.tsv\n",
      "***************\n",
      "prot_t5_xl_uniref50\n",
      "Binding site\n",
      "Using device: cuda\n",
      "Epoch 1/10 - Validation loss: 0.0401\n",
      "Epoch 2/10 - Validation loss: 0.0405\n",
      "Epoch 3/10 - Validation loss: 0.0408\n",
      "Epoch 4/10 - Validation loss: 0.0417\n",
      "Epoch 5/10 - Validation loss: 0.0424\n",
      "Epoch 6/10 - Validation loss: 0.0423\n",
      "Epoch 7/10 - Validation loss: 0.0432\n",
      "Epoch 8/10 - Validation loss: 0.0434\n",
      "Epoch 9/10 - Validation loss: 0.0438\n",
      "Epoch 10/10 - Validation loss: 0.0444\n",
      "Best model saved to ../res/models/lp_BINDING_prot_t5_xl_uniref50.pt with val loss: 0.0401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/423788/ipykernel_942390/1553744952.py:80: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(save_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics saved to ../res/metrics/lp_BINDING_prot_t5_xl_uniref50.tsv\n",
      "***************\n",
      "prot_t5_xl_uniref50\n",
      "DNA binding\n",
      "Using device: cuda\n",
      "Epoch 1/10 - Validation loss: 0.1702\n",
      "Epoch 2/10 - Validation loss: 0.1885\n",
      "Epoch 3/10 - Validation loss: 0.1919\n",
      "Epoch 4/10 - Validation loss: 0.2089\n",
      "Epoch 5/10 - Validation loss: 0.2123\n",
      "Epoch 6/10 - Validation loss: 0.2245\n",
      "Epoch 7/10 - Validation loss: 0.2440\n",
      "Epoch 8/10 - Validation loss: 0.2328\n",
      "Epoch 9/10 - Validation loss: 0.2641\n",
      "Epoch 10/10 - Validation loss: 0.2594\n",
      "Best model saved to ../res/models/lp_DNA_BIND_prot_t5_xl_uniref50.pt with val loss: 0.1702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/423788/ipykernel_942390/1553744952.py:80: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(save_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics saved to ../res/metrics/lp_DNA_BIND_prot_t5_xl_uniref50.tsv\n",
      "***************\n",
      "prot_t5_xl_uniref50\n",
      "Topological domain\n",
      "Using device: cuda\n",
      "Epoch 1/10 - Validation loss: 0.1837\n",
      "Epoch 2/10 - Validation loss: 0.1838\n",
      "Epoch 3/10 - Validation loss: 0.1934\n",
      "Epoch 4/10 - Validation loss: 0.1880\n",
      "Epoch 5/10 - Validation loss: 0.1881\n",
      "Epoch 6/10 - Validation loss: 0.1873\n",
      "Epoch 7/10 - Validation loss: 0.1842\n",
      "Epoch 8/10 - Validation loss: 0.1909\n",
      "Epoch 9/10 - Validation loss: 0.1903\n",
      "Epoch 10/10 - Validation loss: 0.1872\n",
      "Best model saved to ../res/models/lp_TOPO_DOM_prot_t5_xl_uniref50.pt with val loss: 0.1837\n",
      "Test metrics saved to ../res/metrics/lp_TOPO_DOM_prot_t5_xl_uniref50.tsv\n",
      "***************\n",
      "prot_t5_xl_uniref50\n",
      "Transmembrane\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/423788/ipykernel_942390/1553744952.py:80: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(save_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/10 - Validation loss: 0.1367\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#for checkpoint in ESMs + ProtT5:\n",
    "for checkpoint in ProtT5:\n",
    "    for i in range(len(all_features)):\n",
    "        print(checkpoint)\n",
    "        print(all_features[i])\n",
    "        \n",
    "        #read_train_test_val embeds\n",
    "        train_df = pd.read_csv(f\"../data/splits/df/{all_features[i]}_train.tsv.gz\", sep='\\t')[['Entry', 'Sequence', all_features[i]]]\n",
    "        val_df = pd.read_csv(f\"../data/splits/df/{all_features[i]}_val.tsv.gz\", sep='\\t')[['Entry', 'Sequence', all_features[i]]]\n",
    "        test_df = pd.read_csv(f\"../data/splits/df/{all_features[i]}_test.tsv.gz\", sep='\\t')[['Entry', 'Sequence', all_features[i]]]\n",
    "        \n",
    "        train_embeds = [np.load(f'../data/embeddings/{f}_{checkpoint}.npy') for f in train_df.Entry.tolist() if f not in long_proteins]\n",
    "        train_embeds = np.concatenate(train_embeds, axis=0)  \n",
    "\n",
    "        val_embeds = [np.load(f'../data/embeddings/{f}_{checkpoint}.npy') for f in val_df.Entry.tolist() if f not in long_proteins]\n",
    "        val_embeds = np.concatenate(val_embeds, axis=0)  \n",
    "\n",
    "        test_embeds = [np.load(f'../data/embeddings/{f}_{checkpoint}.npy') for f in test_df.Entry.tolist() if f not in long_proteins]\n",
    "        test_embeds = np.concatenate(test_embeds, axis=0)  \n",
    "\n",
    "        # create labels\n",
    "        if all_features[i] == 'Disulfide bond':\n",
    "            labeler_func = build_labels_bonds\n",
    "        else:\n",
    "            labeler_func = build_labels_region\n",
    "            \n",
    "        train_labels = []\n",
    "        for row_idx, row in train_df.iterrows():\n",
    "            if row['Entry'] not in long_proteins:\n",
    "                row_labels = labeler_func(row[\"Sequence\"], row[all_features[i]], all_features_re[i])\n",
    "                train_labels.append(row_labels)\n",
    "        train_labels = np.concatenate(train_labels, axis=0)\n",
    "        \n",
    "        test_labels = []\n",
    "        for row_idx, row in test_df.iterrows():\n",
    "            if row['Entry'] not in long_proteins:\n",
    "                row_labels = labeler_func(row[\"Sequence\"], row[all_features[i]], all_features_re[i])\n",
    "                test_labels.append(row_labels)\n",
    "        test_labels = np.concatenate(test_labels, axis=0)\n",
    "        \n",
    "        val_labels = []\n",
    "        for row_idx, row in val_df.iterrows():\n",
    "            if row['Entry'] not in long_proteins:\n",
    "                row_labels = labeler_func(row[\"Sequence\"], row[all_features[i]], all_features_re[i])\n",
    "                val_labels.append(row_labels)\n",
    "        val_labels = np.concatenate(val_labels, axis=0)\n",
    "        \n",
    "        #train and evaluate the model\n",
    "        model, metrics = train_predictor(\n",
    "                    train_embeds, train_labels,\n",
    "                    val_embeds, val_labels,\n",
    "                    test_embeds, test_labels,\n",
    "                    save_path=f\"../res/models/lp_{all_features_re[i]}_{checkpoint}.pt\",\n",
    "                    metric_path=f\"../res/metrics/lp_{all_features_re[i]}_{checkpoint}.tsv\"\n",
    "                )\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"***************\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
